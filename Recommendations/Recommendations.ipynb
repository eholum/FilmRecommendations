{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from scikits.crab.models.classes import MatrixPreferenceDataModel, MatrixBooleanPrefDataModel\n",
    "from scikits.crab.metrics import *\n",
    "from scikits.crab.similarities import UserSimilarity\n",
    "from scikits.crab.recommenders.knn import UserBasedRecommender\n",
    "from scikits.crab.recommenders.svd.classes import MatrixFactorBasedRecommender\n",
    "from scikits.crab.recommenders.knn.item_strategies import AllPossibleItemsStrategy, ItemsNeighborhoodStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary Data Holder for Reviews Information\n",
    "The reviews dataset allows us to load and pass information to Crab to be analyzed. As well let's us remove and add data as needed for producing personalized recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class reviews_dataset():\n",
    "    '''\n",
    "    Specialized class for reading recommendations csv files produced\n",
    "    by our rotten tomatoes parsing script.\n",
    "    \n",
    "    Implements the Crab (http://muricoca.github.io/crab/index.html)\n",
    "    interface to be used with their recommendation system implementations\n",
    "    \n",
    "    namely:\n",
    "    data - mapping of user ids to preferences\n",
    "    user_ids - mapping of reviewer names to assigned id\n",
    "    item_ids - mapping of movie ids to their titles\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, fileloc):\n",
    "        \n",
    "        self.data = {}\n",
    "        \n",
    "        self.item_ids = {}\n",
    "        self.movie_ids = {}\n",
    "        \n",
    "        self.user_ids = set()\n",
    "        \n",
    "        self.invalid_scores = []        \n",
    "        self.reviews = []\n",
    "        \n",
    "        with open(fileloc) as f:\n",
    "            reader = csv.reader(f, delimiter='\\t', quotechar='\\\"')\n",
    "            \n",
    "            for row in reader:\n",
    "                self.__append_row(row)\n",
    "                \n",
    "    def __append_row(self, row):\n",
    "        name = row[0]\n",
    "        mid = int(row[1])       \n",
    "        title = row[2]\n",
    "        score = row[6]\n",
    "        \n",
    "        try:\n",
    "            # Normalize score to a 1-5 scale\n",
    "            score = float(score)\n",
    "            score = 1 + (score*4)\n",
    "        except:\n",
    "            self.invalid_scores.append(row)\n",
    "            return\n",
    "                \n",
    "        self.reviews.append(row)\n",
    "        \n",
    "        self.user_ids.add(name)\n",
    "        \n",
    "        self.item_ids[mid] = title\n",
    "        if title not in self.movie_ids:\n",
    "            self.movie_ids[title] = set()\n",
    "        self.movie_ids[title].add(mid)\n",
    "        \n",
    "        if not name in self.data:\n",
    "            self.data[name] = {}\n",
    "        self.data[name][mid] = score\n",
    "        \n",
    "    def get_review_counts(self):\n",
    "        \"\"\"\n",
    "        Return a list of movie ids, sorted by the highest number of \n",
    "        review counts.\n",
    "        \"\"\"\n",
    "        movies = self.data\n",
    "\n",
    "        # Get the reviews with the most data in them\n",
    "        counts = {}\n",
    "        for rev in movies:\n",
    "            for mid in movies[rev]:\n",
    "                if not mid in counts:\n",
    "                    counts[mid] = 0\n",
    "                counts[mid] += 1\n",
    "            \n",
    "        return sorted(counts.items(), key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    def get_score_counts(self):\n",
    "        \"\"\"\n",
    "        Return the number of non-empty cells in the data dictionary\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for name in self.data:\n",
    "            count += len(self.data[name])\n",
    "        name\n",
    "    \n",
    "    def get_title(self, mid):\n",
    "        \"\"\"\n",
    "        Returns title string for movie id\n",
    "        \"\"\"\n",
    "        return self.item_ids[mid]\n",
    "    \n",
    "    def get_mid(self, title):\n",
    "        \"\"\"\n",
    "        Returns the movie id given the title of the movie (in a list) since\n",
    "        there may be more than one mid per title.\n",
    "        \"\"\"\n",
    "        return self.movie_ids[title]\n",
    "    \n",
    "    def add_reviewer(self, name, scores):\n",
    "        \"\"\"\n",
    "        Given a name and scores, add a reviewer to this dataset\n",
    "        \n",
    "        Note: This will overwrite any data currently in place!\n",
    "        \"\"\"\n",
    "        self.remove_reviewer(name)\n",
    "        \n",
    "        self.data[name] = scores\n",
    "        self.user_ids.add(name)\n",
    "        \n",
    "    def remove_reviewer(self, name):\n",
    "        \"\"\"\n",
    "        Removes and returns the scores for a given reviewer (in the case of saving them)\n",
    "        \"\"\"\n",
    "        if name not in self.data:\n",
    "            return None\n",
    "        \n",
    "        scores = self.data[name]\n",
    "        del(self.data[name])\n",
    "        self.user_ids.remove(name)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def remove_movie(self, mid):\n",
    "        \"\"\"\n",
    "        Removes and returns ratings for a given movie.\n",
    "        \"\"\"\n",
    "        if mid not in self.item_ids:\n",
    "            return None\n",
    "        \n",
    "        scores = {}\n",
    "        for name in self.data:\n",
    "            if mid in self.data[name]:\n",
    "                scores[name] = self.data[name][mid]\n",
    "                del(self.data[name][mid])\n",
    "        \n",
    "        title = self.item_ids[mid]\n",
    "        del(self.item_ids[mid])\n",
    "        self.movie_ids[title].remove(mid)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def add_movie(self, mid, title, scores):\n",
    "        \"\"\"\n",
    "        given id, title, and scores, add them to the data set for this object\n",
    "        \n",
    "        Note: This will overwrite any existing data!\n",
    "        \"\"\"\n",
    "        self.remove_movie(mid)\n",
    "        \n",
    "        for name in scores:\n",
    "            self.data[name][mid] = scores[name]\n",
    "        \n",
    "        self.item_ids[mid] = title\n",
    "        \n",
    "        if title not in self.movie_ids:\n",
    "            self.movie_ids[title] = set()\n",
    "        self.movie_ids[title].add(mid)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Basic Functional Testing\n",
    "\n",
    "To make sure that everything in the previous class works. We set up a small movie set with 6 movies and a few hundred reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loc = \"/Users/eho/Documents/HES/Machine Learning/Final/Data/test.csv\"\n",
    "mvs = reviews_dataset(test_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{91976: 'Grey, The',\n",
       " 93840: 'Cabin in the Woods, The',\n",
       " 103249: 'World War Z',\n",
       " 104272: 'Blackfish',\n",
       " 107069: 'Lone Survivor',\n",
       " 109895: 'Bad Words'}"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.data['A.O. Scott'][91976]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{103249}"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.get_mid(\"World War Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(93840, 199),\n",
       " (103249, 197),\n",
       " (107069, 159),\n",
       " (91976, 150),\n",
       " (104272, 97),\n",
       " (109895, 95)]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.get_review_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ao_scott_scores = mvs.remove_reviewer(\"A.O. Scott\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{91976: 3.4, 103249: 3.4}"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ao_scott_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"A.O. Scott\" in mvs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mvs.add_reviewer(\"A.O. Scott\", ao_scott_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"A.O. Scott\" in mvs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "revs = mvs.remove_movie(93840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(revs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{91976: 'Grey, The',\n",
       " 103249: 'World War Z',\n",
       " 104272: 'Blackfish',\n",
       " 107069: 'Lone Survivor',\n",
       " 109895: 'Bad Words'}"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.add_movie(93840, \"Cabin in the Woods, The\", revs)\n",
    "len(mvs.item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{91976: 'Grey, The',\n",
       " 93840: 'Cabin in the Woods, The',\n",
       " 103249: 'World War Z',\n",
       " 104272: 'Blackfish',\n",
       " 107069: 'Lone Survivor',\n",
       " 109895: 'Bad Words'}"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.item_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Crab\n",
    "\n",
    "It took a fair bit of time to get crab working. It's documentation is woefully lacking, and there are a few spots in the code that have typos in them still. That being said, we found both the neighbors model and the matrix model to be adequate implementations of both systems. We use our test data set here to make sure everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixPreferenceDataModel (420 by 6)\n",
      "         91976      93840      103249     104272     107069   ...\n",
      "A.A. Dow    ---     4.200000   3.000000      ---     3.000000\n",
      "A.O. Sco 3.400000      ---     3.400000      ---        ---\n",
      "Adam Ros    ---     4.200000      ---        ---        ---\n",
      "Al Alexa    ---        ---        ---        ---     3.800000\n",
      "Alan Jon    ---     4.200000      ---        ---        ---\n",
      "Alex Zan    ---     5.000000      ---        ---        ---\n",
      "Ali Gray    ---     5.000000      ---        ---        ---\n",
      "Alistair    ---        ---        ---     4.200000      ---\n",
      "Amber Wi    ---        ---        ---     4.600000      ---\n",
      "Amy Bian 2.000000      ---        ---        ---        ---\n",
      "Amy Curt    ---     4.000000      ---        ---        ---\n",
      "Amy Nich    ---        ---        ---        ---     2.600000\n",
      "Anders W    ---        ---     2.600000      ---     2.600000\n",
      "Andrea C 4.200000      ---     2.600000      ---        ---\n",
      "Andy Lea    ---     4.200000   4.200000      ---     4.200000\n",
      "Ann Horn    ---     4.000000   3.500000   4.500000      ---\n",
      "Ann Lewi    ---     4.000000      ---        ---        ---\n",
      "Anna Tat    ---        ---        ---        ---        ---\n",
      "Anthony     ---     3.400000      ---     4.200000      ---\n",
      "Anton Bi    ---     4.200000      ---        ---        ---\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "model = MatrixPreferenceDataModel(mvs.data)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_recommendations(reviews, recommender, userid):\n",
    "    recs = recommender.recommend(userid)\n",
    "    return [(reviews.get_title(x[0]), x[1]) for x in recs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_similar_users(reviews, recommender, userid, n=15):\n",
    "    return recommender.most_similar_users(userid, how_many=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There are many other options for distance metrics other available in\n",
    "\n",
    "https://github.com/muricoca/crab/blob/master/scikits/crab/metrics/pairwise.py\n",
    "\"\"\"\n",
    "users_similarity = UserSimilarity(model, pairwise.cosine_distances)\n",
    "recommender = UserBasedRecommender(model, users_similarity, with_preference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{91976: 3.4, 103249: 3.4}"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvs.data[\"A.O. Scott\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{91976: 3.4, 103249: 3.4}\n",
      "[('Blackfish', 4.1314285714285708), ('Cabin in the Woods, The', 4.0886614173228342), ('Lone Survivor', 3.659259259259259), ('Bad Words', 3.5019047619047625)]\n"
     ]
    }
   ],
   "source": [
    "user = \"A.O. Scott\"\n",
    "print(mvs.data[user])\n",
    "print(get_recommendations(mvs, recommender, user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Brad Miska', 'Edward Douglas', 'Jordan Farley'], \n",
       "      dtype='|S14')"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_users(mvs, recommender, user, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# So Does This Actually Scale?\n",
    "We tried applying it to the full dataset of reviews, but it was painfully slow (the neighbors method especially was non-functional). So we cut the dataset down to only look at recent reviews from 2012, 2013, 2014, and 2015. The matrix model was also a drastic improvement in both efficiency and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc = \"/Users/eho/Documents/HES/Machine Learning/Final/Data/recent_reviews.csv\"\n",
    "all_mvs = reviews_dataset(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixPreferenceDataModel (1987 by 2206)\n",
      "         89745      91485      91500      91529      91535    ...\n",
      "A.A. Dow    ---        ---        ---        ---        ---\n",
      "A.O. Sco 0.400000      ---        ---        ---        ---\n",
      "AP Kryza    ---        ---        ---        ---        ---\n",
      "AV Club     ---        ---        ---        ---        ---\n",
      "Aaron Ar    ---        ---        ---        ---        ---\n",
      "Aaron Me    ---        ---        ---        ---        ---\n",
      "Aaron Ne    ---        ---        ---        ---        ---\n",
      "Aaron Ya    ---     0.400000      ---        ---     0.400000\n",
      "Aarti Jh    ---        ---        ---        ---        ---\n",
      "Abbey Be    ---        ---        ---        ---        ---\n",
      "Abby Gar    ---        ---        ---        ---        ---\n",
      "Abby Wes    ---        ---        ---        ---        ---\n",
      "Abhimany    ---        ---        ---        ---        ---\n",
      "Abhinav     ---        ---        ---        ---        ---\n",
      "Abigail     ---        ---        ---        ---        ---\n",
      "Adam A.     ---        ---        ---        ---        ---\n",
      "Adam Bub 0.900000      ---     0.900000   0.900000      ---\n",
      "Adam Cha    ---        ---        ---        ---        ---\n",
      "Adam Coo    ---        ---        ---        ---        ---\n",
      "Adam DiL    ---        ---        ---        ---        ---\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "model = MatrixPreferenceDataModel(all_mvs.data)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# users_similarity = UserSimilarity(model, pairwise.euclidean_distances)\n",
    "# recommender = UserBasedRecommender(model, users_similarity, with_preference=True)\n",
    "recommender = MatrixFactorBasedRecommender(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57882195445260931"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec = recommender.estimate_preference(\"A.O. Scott\", 91485)\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I need to rate some movies!\n",
    "\n",
    "I wanted personalized recommendations, so I just rated some of the movies in the dataset to see how it did for my preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = all_mvs.get_review_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rate_some_movies(reviews, my_scores={}):\n",
    "    \"\"\"\n",
    "    If you don't have any ratings, use this to start ranking movies in order\n",
    "    by review count. Can pass in a filled dictionary if you have already done\n",
    "    this for some movies.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Input movie ratings 0 - 5 (enter nothing to continue or enter any other number to exit)\")\n",
    "        \n",
    "    score = 0\n",
    "    \n",
    "    for x in reviews.get_review_counts():\n",
    "        if x[0] in my_scores:\n",
    "            continue\n",
    "            \n",
    "        print(\"%s: \" % all_mvs.get_title(x[0]))\n",
    "        \n",
    "        score = raw_input()\n",
    "        if not score:\n",
    "            continue\n",
    "        \n",
    "        score = int(score)\n",
    "        if score < 0 or score > 5:\n",
    "            break\n",
    "            \n",
    "        score = score/5.0\n",
    "        \n",
    "        my_scores[x[0]] = score\n",
    "        \n",
    "    return my_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scores = rate_some_movies(all_mvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Commented out to not overwrite\n",
    "#f = open(\"eriks_reviews.pickle\", 'wb')\n",
    "#pickle.dump(scores, f)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Did this previously and wrote my ratings to a file\n",
    "eriks_scores = pickle.load(open(\"eriks_reviews.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_mvs.data[\"Erik Holum\"] = eriks_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I rated 76 movies\n",
    "len(eriks_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing the Recommendations\n",
    "\n",
    "So looking below it takes about 90 seconds to perform the matrix factorization for the data we currently have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.65814399719\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = MatrixPreferenceDataModel(all_mvs.data)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.8303189278\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "my_recommender = MatrixFactorBasedRecommender(model,\n",
    "                                             items_selection_strategy=ItemsNeighborhoodStrategy,\n",
    "                                             n_features=20,\n",
    "                                             learning_rate=.01,\n",
    "                                             n_interations=50,\n",
    "                                             with_preference=True)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['112623 Dawn of the Planet of the Apes',\n",
       " '103228 Pacific Rim',\n",
       " '97752 Cloud Atlas',\n",
       " '117529 Jurassic World',\n",
       " '110102 Captain America: The Winter Soldier',\n",
       " '98961 Zero Dark Thirty',\n",
       " '96610 Looper',\n",
       " '102445 Star Trek Into Darkness',\n",
       " '93840 Cabin in the Woods, The',\n",
       " '102407 Great Gatsby, The']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = [\"%d %s\" % (x[0],all_mvs.get_title(x[0])) for x in all_mvs.get_review_counts()]\n",
    "c = 20\n",
    "movies[c:c+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Wars VII: The Force Awakens 0.747579\n",
      "Sisters 0.652037\n",
      "Peanuts Movie, The 0.676708\n",
      "Son of Saul 0.704667\n",
      "Hunger Games Mockingjay Part 2, The 0.800000\n",
      "Good Dinosaur, The 0.627532\n",
      "Creed 0.713339\n",
      "Krampus 0.566268\n",
      "Spectre 0.400000\n",
      "Hateful Eight, The 0.626948\n",
      "In the Heart of the Sea 0.560637\n",
      "Martian, The 1.000000\n",
      "Pan 0.516011\n",
      "Last Witch Hunter, The 0.418982\n"
     ]
    }
   ],
   "source": [
    "# I've tested this with a bunch of movies that I've seen, and it's never \n",
    "# off by more than one star. So that's cool!!!\n",
    "for i in range(10000000, 10000014):\n",
    "    print(\"%s %f\" % (all_mvs.get_title(i), my_recommender.estimate_preference(\"Erik Holum\", i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Verification\n",
    "\n",
    "We use the standard but controversial Root Mean Squared Error to evaluate our recommender's performance. We start off just by splitting the scraped scores into crossvalidation sets, and found that the RMSE was .63! Unsurprising since we used professionally produced data on a small set, but it makes sense that reviewers' preferences would be more accurate over time than the average users. As we expect them to be more \"consistent\" in their ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc = \"/Users/eho/Documents/HES/Machine Learning/Final/Data/recent_reviews.csv\"\n",
    "all_mvs = reviews_dataset(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447\n"
     ]
    }
   ],
   "source": [
    "# Find reviewers with more than 50 reviews\n",
    "reviewers = []\n",
    "for name in all_mvs.user_ids:\n",
    "    if len(all_mvs.data[name]) > 50:\n",
    "        reviewers.append(name)\n",
    "print(len(reviewers))\n",
    "\n",
    "# choose a selection of them at random\n",
    "np.random.seed(1000)\n",
    "reviewers = np.random.choice(reviewers, size=100, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 2, 3: 4, 4: 5, 5: 6, 7: 8, 9: 10}\n",
      "{8: 9, 2: 3, 6: 7}\n"
     ]
    }
   ],
   "source": [
    "def split_dictionary(dictionary, size=.3, random=None):\n",
    "    \"\"\"\n",
    "    Randomly split a dictionary into two parts of size and 1 - size.\n",
    "    \n",
    "    Return deep copies of the new dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    tmp = dictionary.keys()\n",
    "    \n",
    "    np.random.seed(random)\n",
    "    keys = np.random.choice(tmp, size=int(len(tmp)*size), replace=False)\n",
    "\n",
    "    newdict = {}\n",
    "    olddict = copy.deepcopy(dictionary)\n",
    "    for k in keys:\n",
    "        newdict[k] = olddict[k]    \n",
    "        del(olddict[k])\n",
    "        \n",
    "    return [olddict, newdict]\n",
    "\n",
    "d = {a:a+1 for a in range(10)}\n",
    "old, new = split_dictionary(d, size=.3, random=None)\n",
    "print(old)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_reviewers(names, test_scores=None, mvs_set=None, cv_size=.3, random_state=None, fileloc=None):\n",
    "    \"\"\"\n",
    "    Given the names of a set of reviewers, splits their review data into cross-validation set and\n",
    "    a training set, builds a cf model using all other data, then evaluates the predictions\n",
    "    produced for the CV set.\n",
    "    \n",
    "    Returns 1-D arrays of expected and actual results. In order for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Allow specification of reviews file location\n",
    "    if fileloc:\n",
    "        mvs_set = reviews_dataset(loc)\n",
    "        \n",
    "    if not mvs_set:\n",
    "        raise Exception\n",
    "        \n",
    "    # Add scores if specified\n",
    "    if test_scores:\n",
    "        for name in names:\n",
    "            mvs_set.add_reviewer(name, test_scores[name])\n",
    "\n",
    "    # Remove data from data set\n",
    "    all_scores = {}\n",
    "    for name in names:\n",
    "        all_scores[name] = mvs_set.remove_reviewer(name)\n",
    "    \n",
    "    # Split into a CV set\n",
    "    train_set = {}\n",
    "    test_set = {}\n",
    "    for name in names:\n",
    "        train_scores, test_scores = split_dictionary(all_scores[name], \n",
    "                                                     size=cv_size, \n",
    "                                                     random=random_state)\n",
    "        train_set[name] = train_scores\n",
    "        test_set[name] = test_scores\n",
    "    \n",
    "    # Add the training set to the data set\n",
    "    for name in names:\n",
    "        mvs_set.add_reviewer(name, train_set[name])\n",
    "    \n",
    "    try :\n",
    "        # Perform the matrix factorization\n",
    "        # The real problem with crab is that this has to be recomputed every time!\n",
    "        # We note that we have a problem with \"overfitting\" and hence limit the iterations to 30.\n",
    "        model = MatrixPreferenceDataModel(mvs_set.data)\n",
    "        my_recommender = MatrixFactorBasedRecommender(model,\n",
    "                                                      items_selection_strategy=ItemsNeighborhoodStrategy,\n",
    "                                                      n_features=20,\n",
    "                                                      learning_rate=.01,\n",
    "                                                      n_interations=30,\n",
    "                                                      with_preference=True)\n",
    "\n",
    "        # Evaluate the test scores\n",
    "        actual_scores = []\n",
    "        predicted_scores = []\n",
    "        revs = []\n",
    "        movies = []\n",
    "        \n",
    "        # For each name and each left out rating, get the scores and append them to our results\n",
    "        for name in names:\n",
    "            cv = test_set[name]\n",
    "            for mid in cv:\n",
    "                if mid not in model.item_ids():\n",
    "                    # This can happen if we remove the only review for that movie. In that\n",
    "                    # case we're not that interested anyway.\n",
    "                    continue\n",
    "                actual_scores.append(cv[mid])\n",
    "                predicted_scores.append(my_recommender.estimate_preference(name, mid))\n",
    "                revs.append(name)\n",
    "                movies.append([mid, mvs_set.get_title(mid)])\n",
    "                \n",
    "    finally:\n",
    "        # No matter what be sure we put the data back\n",
    "        for name in names:\n",
    "            mvs_set.add_reviewer(name, all_scores[name])\n",
    "            \n",
    "    return [revs, movies, actual_scores, predicted_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.7227740288\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "results = evaluate_reviewers(reviewers, \n",
    "                             fileloc=\"/Users/eho/Documents/HES/Machine Learning/Final/Data/recent_reviews.csv\", \n",
    "                             cv_size=.2, \n",
    "                             random_state=10000)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6306454695244874"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shocking? Maybe not since we are comparing netflix reviews to netflix reviews. Recall that the prize for\n",
    "# the netflix competition had an RMSE of .85\n",
    "math.sqrt(mean_squared_error(results[2], results[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual User Ratings Comparison\n",
    "\n",
    "The 550 MB ratings file in ml-20 has far too many movies for us to actually evaluate in a simple iPython notebook. Here we are filtering out the reviews for only the movies in our all_mvs set. We still end up with 181122 ratings, more than we are interested in using now. We select a random set of 650 users, then split each set into a training set and a test set for cross-validation.\n",
    "\n",
    "Using a set of 3696 reviews, we found our RMSE to be .867, so less than a star rating off. The next step would be to build a matrix factorization algorithm that actually produces results in a more efficient manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copy only movies that we are interested in\n",
    "f = open(\"/Users/eho/Documents/HES/Machine Learning/movies/ml-20m/ratings.csv\", \"r\")\n",
    "o = open(\"test_ratings.csv\", \"w\")\n",
    "f.readline()\n",
    "for line in f:\n",
    "    mid = int(line.split(',')[1])\n",
    "    if mid in all_mvs.item_ids:\n",
    "        o.write(line)\n",
    "        \n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12602"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings = {}\n",
    "ratings_file = open(\"test_ratings.csv\", \"r\")\n",
    "\n",
    "for line in ratings_file:\n",
    "    l = line.split(',')\n",
    "    userId = str(l[0])\n",
    "    mid = int(l[1])\n",
    "    score = float(l[2])\n",
    "    \n",
    "    if userId not in user_ratings:\n",
    "        user_ratings[userId] = {}\n",
    "    \n",
    "    user_ratings[userId][mid] = score\n",
    "\n",
    "len(user_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6705"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 120602 is a lot, lets remove the empty ones and ones with less than 5 (there are many)\n",
    "tmp = user_ratings.keys()\n",
    "for uid in tmp:\n",
    "    if len(user_ratings[uid]) < 5:\n",
    "        del(user_ratings[uid])\n",
    "len(user_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_user_ratings = split_dictionary(user_ratings, size=.1, random=1000)[1]\n",
    "len(test_user_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uids = []\n",
    "for uid in test_user_ratings:\n",
    "    uids.append(uid)\n",
    "    all_mvs.add_reviewer(uid, test_user_ratings[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782.425244093\n"
     ]
    }
   ],
   "source": [
    "# Run our tests, do this in sets of 50, assuming that that many uids won't disrupt the model (since there are ~2000\n",
    "# professionals)\n",
    "start = time.time()\n",
    "\n",
    "ratings_results = []\n",
    "\n",
    "for i in range(0, 550, 50):\n",
    "    tmp_users = uids[i:i+50]\n",
    "    tmp_scores = {uid:test_user_ratings[uid] for uid in tmp_users}\n",
    "    ratings_results.append(evaluate_reviewers(tmp_users, \n",
    "                           fileloc=\"/Users/eho/Documents/HES/Machine Learning/Final/Data/recent_reviews.csv\", \n",
    "                           cv_size=.3, \n",
    "                           random_state=10000,\n",
    "                           test_scores=tmp_scores))\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_results = [[],[],[],[]]\n",
    "for r in ratings_results:\n",
    "    all_results[0] += r[0]\n",
    "    all_results[1] += r[1]\n",
    "    all_results[2] += r[2]\n",
    "    all_results[3] += r[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8655136031204704"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .86 isn't too shabby. It's a small test set, but we haven't done a lot of work for this. I think it shows promise\n",
    "math.sqrt(mean_squared_error(all_results[2], all_results[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3696"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With 3696 test scores our RSME was .866\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(all_results, open(\"all_ratings_results.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_results = pickle.load(open(\"all_ratings_results.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5052290928927783"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dumping truth and predictions\n",
    "f = open(\"all_ratings_results.csv\", \"w\")\n",
    "for i in range(len(all_results[0])):\n",
    "    f.write(\"%f,%f\\n\" % (all_results[2][i],\n",
    "                               all_results[3][i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Will People Like Star Wars?\n",
    "\n",
    "I set up a google forum to gather reviews from friends, family, and classmates. Just to let them know what to expect when choosing whether to wait in line for star wars, or to give Sisters a much needed extra $10 on their opening weekend.\n",
    "\n",
    "Surprisingly, according to our recommender there are a few people who shouldn't spend the money on Star Wars (including my mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc = \"/Users/eho/Documents/HES/Machine Learning/Final/Data/classmate_ratings.csv\"\n",
    "reader = csv.reader(open(loc), delimiter=',', quotechar='\\\"')\n",
    "\n",
    "class_scores = {}\n",
    "\n",
    "header = reader.next()[2:]\n",
    "movies = [copy.deepcopy(all_mvs.get_mid(x)) for x in header]\n",
    "movies[0].add(10000005)\n",
    "movies[1].add(10000011)\n",
    "movies[2].add(10000013)\n",
    "\n",
    "for line in reader:\n",
    "    name = line[1]\n",
    "    scores = {}\n",
    "    \n",
    "    for i in range(len(movies)):\n",
    "        mid = movies[i].pop()\n",
    "        movies[i].add(mid)\n",
    "        s = line[i+2]\n",
    "        try:\n",
    "            scores[mid] = float(s)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    class_scores[name] = scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc = \"/Users/eho/Documents/HES/Machine Learning/Final/Data/recent_reviews.csv\"\n",
    "all_mvs = reviews_dataset(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in class_scores:\n",
    "    all_mvs.add_reviewer(c, class_scores[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MatrixPreferenceDataModel(all_mvs.data)\n",
    "my_recommender = MatrixFactorBasedRecommender(model,\n",
    "                                              items_selection_strategy=ItemsNeighborhoodStrategy,\n",
    "                                              n_features=20,\n",
    "                                              learning_rate=.01,\n",
    "                                              n_interations=30,\n",
    "                                              with_preference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarah 4.532940 3.815769\n",
      "ABC 2.723338 3.243975\n",
      "Ankit 3.791182 3.606406\n",
      "Nick 4.004851 3.443085\n",
      "merri.huang@gmail.com 4.023050 3.476290\n",
      "Luke 3.177488 3.428144\n",
      "Dan 4.212531 3.629121\n",
      "Gaby 3.568885 3.399609\n",
      "Britta 4.449709 3.598436\n",
      "Moma G 4.051785 3.514519\n",
      "MT 3.686679 3.404661\n",
      "Movie H8er 0.735812 2.754545\n",
      "chollum@msn.com 3.748584 3.444994\n",
      "Nils 4.885844 3.832616\n",
      "Linda 2.914382 3.145942\n"
     ]
    }
   ],
   "source": [
    "sw = 10000000\n",
    "sis = 10000001\n",
    "\n",
    "for c in class_scores:\n",
    "    sisters = my_recommender.estimate_preference(c, sis)\n",
    "    starwars = my_recommender.estimate_preference(c, sw)\n",
    "    print(\"%s %f %f\" % (c, starwars, sisters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
